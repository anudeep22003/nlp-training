{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Earley Chart Parser.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNWIAZEm1kD2Xmo1EGpugIk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anudeep22003/nlp-training/blob/main/Earley_Chart_Parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3_qjI14OhJ1"
      },
      "source": [
        "import nltk\n",
        "from pprint import pprint\n",
        "from collections import defaultdict"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUDJgIdwPbDd"
      },
      "source": [
        "Initilize an entire leaf table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASVYz9BSPYzR",
        "outputId": "9a62523e-6693-4ba0-cace-0b9daf3aada4"
      },
      "source": [
        "def init_leaftable(tokens):\n",
        "  table = [[None for b in range(len(tokens)+1)] for l in range(len(tokens))]\n",
        "  for depth in range(len(tokens)):\n",
        "    \n",
        "    for length in range(len(tokens)+1):\n",
        "      # print(length)\n",
        "      if length == depth+1:\n",
        "        table[depth][length]= tokens[depth]\n",
        "      else:\n",
        "        table[depth][length] = \"----\"\n",
        "  \n",
        "  return table\n",
        "\n",
        "def display(wfst):\n",
        "    print('\\nWFST ' + ' '.join((\"%-4d\" % i) for i in range(1, len(wfst[0]))))\n",
        "    for i in range(len(wfst)):\n",
        "        print(\"%d   \" % i, end=\" \")\n",
        "        for j in range(1, len(wfst)):\n",
        "            print(\"%-4s\" % (wfst[i][j] or '.'), end=\" \")\n",
        "        print()\n",
        "\n",
        "\n",
        "sent = \"I saw John with a dog\".split()\n",
        "\n",
        "wfst = init_leaftable(sent)\n",
        "print(len(wfst))\n",
        "pprint(wfst)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "[['----', 'I', '----', '----', '----', '----', '----'],\n",
            " ['----', '----', 'saw', '----', '----', '----', '----'],\n",
            " ['----', '----', '----', 'John', '----', '----', '----'],\n",
            " ['----', '----', '----', '----', 'with', '----', '----'],\n",
            " ['----', '----', '----', '----', '----', 'a', '----'],\n",
            " ['----', '----', '----', '----', '----', '----', 'dog']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3gHD4Ykb7rH"
      },
      "source": [
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "\n",
        "    S -> NP VP\n",
        "    PP -> 'with' NP\n",
        "    NP -> NP PP\n",
        "    VP -> VP PP\n",
        "    VP -> Verb NP\n",
        "    VP -> Verb\n",
        "    NP -> Det Noun\n",
        "    NP -> 'John'\n",
        "    NP -> 'I'\n",
        "    Det -> 'the'\n",
        "    Det -> 'my'\n",
        "    Det -> 'a'\n",
        "    Noun -> 'dog'\n",
        "    Noun -> 'cookie'\n",
        "    Verb -> 'ate'\n",
        "    Verb -> 'saw'\n",
        "    Prep -> 'with'\n",
        "    Prep -> 'under'\n",
        "\n",
        "\"\"\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P04nl9HpZDd9",
        "outputId": "afcc6193-8831-4e71-fc09-8ca88910b0db"
      },
      "source": [
        "class EarleyChartParser:\n",
        "  \n",
        "  # initializer function\n",
        "  def __init__(self, sent_string, grammar):\n",
        "    self.sentence_string = sent_string\n",
        "    self.tokens = self.sentence_string.split()\n",
        "    self.grammar = grammar\n",
        "    self.index = defaultdict(list)\n",
        "    \n",
        "    # using this to add active states and grammars\n",
        "    self.active_sent_q = []\n",
        "    self.active_predictor_q = []    \n",
        "    # Start the process of parsing\n",
        "    self.start_recipe()  \n",
        "\n",
        "  # Put the function call order here\n",
        "  def start_recipe(self):\n",
        "    self.dictize()\n",
        "    #print(self.index)\n",
        "    \n",
        "    # this initializes the dot that we will be moving every step\n",
        "    self.active_sent_q = self.dot_sentlist()\n",
        "    \n",
        "    # start the dot on production list\n",
        "    self.active_predictor_q = self.index['S']\n",
        "    \n",
        "    self.predictor()\n",
        "\n",
        "\n",
        "  # create a dict which is an index of all the grammar production rules\n",
        "  def dictize(self):\n",
        "    for p in grammar.productions():\n",
        "      self.index[\"{}\".format(p.lhs())].append(p.rhs())\n",
        "\n",
        "\n",
        "\n",
        "  # start the dot\n",
        "  # initialize the dot list\n",
        "  def dot_sentlist(self):\n",
        "    dotlist = [\".\"]\n",
        "    # dotlist.append(t) for t in token\n",
        "    for token in self.tokens:\n",
        "      dotlist.append(token)\n",
        "    return dotlist\n",
        "    \n",
        "\n",
        "\n",
        "  def output(self):\n",
        "    print(self.tokens)\n",
        "    print(self.dot_list)\n",
        "    pprint(self.list_index)\n",
        "    pprint(self.index)\n",
        "  \n",
        "  \n",
        "  # predictor\n",
        "  # predicts what rules to apply \n",
        "  # specifically looks for the production rules based on the symbol to the right of the dot\n",
        "  # if it finds any, it will add it to the prediction queue \n",
        "  # until it hits a terminal \n",
        "\n",
        "  def predictor(self):\n",
        "    print(self.active_sent_q)\n",
        "    print(self.active_predictor_q)\n",
        "\n",
        "    for p in self.index['{}'.format(self.active_predictor_q[0][0])]:\n",
        "      self.active_predictor_q.append(p)\n",
        "    \n",
        "    print(\"----------------- Step 1 ----------------\")\n",
        "    print(self.active_predictor_q)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "  # scanner \n",
        "  # completor\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "obj = EarleyChartParser(\"I am in love\",grammar)\n",
        "#obj.start()\n",
        "# obj.output()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.', 'I', 'am', 'in', 'love']\n",
            "[(NP, VP)]\n",
            "----------------- Step 1 ----------------\n",
            "[(NP, VP), (NP, PP), (Det, Noun), ('John',), ('I',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJv_FLOMZTmM"
      },
      "source": [
        ">>> class Complex:\n",
        "...     def __init__(self, realpart, imagpart):\n",
        "...         self.r = realpart\n",
        "...         self.i = imagpart"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chgraI4pdB2K",
        "outputId": "78aecee8-197a-48d6-aca3-d031c3e69321"
      },
      "source": [
        "def temp():\n",
        "  a = [1]\n",
        "  return a.append(range(5))\n",
        "\n",
        "a = [1]\n",
        "a.append([i for i in range(5)])\n",
        "print(a)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, [0, 1, 2, 3, 4]]\n"
          ]
        }
      ]
    }
  ]
}